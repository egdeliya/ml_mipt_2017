
[+] General Parameters:
Training data:	train.txt
Validation data:	validation.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@5
Test metric:	NDCG@5
Feature normalization: No
Model file: log_LambdaMART

[+] LambdaMART's Parameters:
No. of trees: 300
No. of leaves: 10
No. of threshold candidates: 256
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [train.txt]: 0... Reading feature file [train.txt]... [Done.]            
(84 ranked lists, 693 entries read)
Reading feature file [validation.txt]: 0... Reading feature file [validation.txt]... [Done.]            
(42 ranked lists, 297 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | NDCG@5-T  | NDCG@5-V  | 
---------------------------------
1       | 0.6901    | 0.7037    | 
2       | 0.7447    | 0.7047    | 
3       | 0.7558    | 0.7164    | 
4       | 0.7494    | 0.7194    | 
5       | 0.7536    | 0.7329    | 
6       | 0.7516    | 0.7273    | 
7       | 0.7562    | 0.7199    | 
8       | 0.7587    | 0.7203    | 
9       | 0.7604    | 0.7263    | 
10      | 0.7607    | 0.7167    | 
11      | 0.7616    | 0.7127    | 
12      | 0.7654    | 0.7121    | 
13      | 0.7674    | 0.707     | 
14      | 0.7667    | 0.7181    | 
15      | 0.7695    | 0.7179    | 
16      | 0.7693    | 0.7243    | 
17      | 0.7726    | 0.725     | 
18      | 0.7751    | 0.7155    | 
19      | 0.7742    | 0.7136    | 
20      | 0.778     | 0.7151    | 
21      | 0.7788    | 0.7053    | 
22      | 0.7779    | 0.7063    | 
23      | 0.7784    | 0.703     | 
24      | 0.7787    | 0.7041    | 
25      | 0.7785    | 0.7088    | 
26      | 0.7786    | 0.7136    | 
27      | 0.78      | 0.7104    | 
28      | 0.7815    | 0.7081    | 
29      | 0.7818    | 0.7089    | 
30      | 0.7841    | 0.7127    | 
31      | 0.7841    | 0.7118    | 
32      | 0.7844    | 0.7089    | 
33      | 0.7868    | 0.7073    | 
34      | 0.7843    | 0.7088    | 
35      | 0.7845    | 0.7025    | 
36      | 0.7845    | 0.7028    | 
37      | 0.7861    | 0.7042    | 
38      | 0.7855    | 0.7028    | 
39      | 0.7854    | 0.7028    | 
40      | 0.7876    | 0.7023    | 
41      | 0.7892    | 0.7082    | 
42      | 0.7894    | 0.7069    | 
43      | 0.7919    | 0.7063    | 
44      | 0.7908    | 0.7082    | 
45      | 0.7925    | 0.7105    | 
46      | 0.7896    | 0.7042    | 
47      | 0.7915    | 0.7011    | 
48      | 0.7911    | 0.7022    | 
49      | 0.7954    | 0.7082    | 
50      | 0.7971    | 0.7064    | 
51      | 0.799     | 0.7064    | 
52      | 0.7957    | 0.7089    | 
53      | 0.7974    | 0.7091    | 
54      | 0.7951    | 0.7044    | 
55      | 0.7979    | 0.7029    | 
56      | 0.7979    | 0.7029    | 
57      | 0.7951    | 0.7032    | 
58      | 0.7965    | 0.7032    | 
59      | 0.7998    | 0.7069    | 
60      | 0.7998    | 0.7069    | 
61      | 0.7998    | 0.7081    | 
62      | 0.7998    | 0.7071    | 
63      | 0.7998    | 0.7054    | 
64      | 0.7998    | 0.7041    | 
65      | 0.7998    | 0.7041    | 
66      | 0.8009    | 0.7044    | 
67      | 0.8012    | 0.7023    | 
68      | 0.8038    | 0.7021    | 
69      | 0.8089    | 0.7013    | 
70      | 0.8108    | 0.6995    | 
71      | 0.81      | 0.7037    | 
72      | 0.8106    | 0.703     | 
73      | 0.8101    | 0.7049    | 
74      | 0.8102    | 0.7032    | 
75      | 0.8099    | 0.7032    | 
76      | 0.8099    | 0.7032    | 
77      | 0.8101    | 0.7028    | 
78      | 0.8107    | 0.7059    | 
79      | 0.8117    | 0.7045    | 
80      | 0.8135    | 0.7007    | 
81      | 0.8133    | 0.7003    | 
82      | 0.8136    | 0.7005    | 
83      | 0.8176    | 0.6974    | 
84      | 0.8194    | 0.6984    | 
85      | 0.8194    | 0.6964    | 
86      | 0.8216    | 0.6964    | 
87      | 0.8217    | 0.694     | 
88      | 0.8215    | 0.6943    | 
89      | 0.8235    | 0.6943    | 
90      | 0.8242    | 0.6953    | 
91      | 0.8242    | 0.6969    | 
92      | 0.8242    | 0.6969    | 
93      | 0.8243    | 0.6963    | 
94      | 0.8239    | 0.6961    | 
95      | 0.8245    | 0.6883    | 
96      | 0.8247    | 0.6851    | 
97      | 0.8247    | 0.6823    | 
98      | 0.8243    | 0.6786    | 
99      | 0.8242    | 0.6834    | 
100     | 0.8245    | 0.6834    | 
101     | 0.824     | 0.6832    | 
102     | 0.8234    | 0.6843    | 
103     | 0.8256    | 0.6843    | 
104     | 0.8256    | 0.6819    | 
105     | 0.8256    | 0.6819    | 
106     | 0.8267    | 0.6827    | 
---------------------------------
Finished sucessfully.
NDCG@5 on training data: 0.7536
NDCG@5 on validation data: 0.7329
---------------------------------

Model saved to: log_LambdaMART

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n",
      "env: DISPLAY=:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"bash\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21a34ad6748>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP6IhlgiOSRGBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21a2b1d2828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fb239e46492d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#create input variables. We'll support multiple states at once\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lasagne'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b1208de5f0f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#input layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ml_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lasagne'"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "\n",
    "# <Your architecture. Please start with a single-layer network>\n",
    "first_layer = DenseLayer(l_states, 100)\n",
    "second_layer = DenseLayer(first_layer, 200)\n",
    "\n",
    "#output layer\n",
    "l_qvalues = DenseLayer(second_layer, num_units=n_actions,nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-45b074a17e76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#get q-values for ALL actions in current_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredicted_qvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_qvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0ml_states\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_output' is not defined"
     ]
    }
   ],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues,{l_states:current_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theano' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f9588e39ca75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#compiling agent's \"GetQValues\" function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_qvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_states\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_qvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# <compile a function that takes current_states and returns predicted_qvalues>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'theano' is not defined"
     ]
    }
   ],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "get_qvalues = theano.function([current_states], T.argmax(predicted_qvalues, axis=1)) \n",
    "# <compile a function that takes current_states and returns predicted_qvalues>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_qvalues' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b506ca62d820>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#select q-values for chosen actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredicted_qvalues_for_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_qvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_qvalues' is not defined"
     ]
    }
   ],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]),actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues,{l_states:next_states})\n",
    "#                                                <theano input with for states>})\n",
    "\n",
    "\n",
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = \n",
    "# <target Q-values using rewards and predicted_next_qvalues>\n",
    "\n",
    "#zero-out q-values at the end\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mean squared error loss function\n",
    "loss = <mean squared between target_qvalues_for_actions and predicted_qvalues_for_actions>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues,trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.sgd(loss,all_weights,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states,actions,rewards,next_states,is_end],\n",
    "                             updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues([s])[0] \n",
    "        \n",
    "        a = <sample action with epsilon-greedy strategy>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step([s],[a],[r],[new_s],[done])\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
